{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a54221-7614-426f-ae15-a371424a8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23461b8c-946e-40d3-83a5-da82a8c2099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26d3dbf-3706-4e2c-8fd5-7d18427518f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smotetomek succesfully imported\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "print(\"Smotetomek succesfully imported\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "231b4003-f8db-4582-b5bb-22ea8ff99dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>Employment_status</th>\n",
       "      <th>Personal_Net_Income_Category</th>\n",
       "      <th>Ethnic_Background</th>\n",
       "      <th>living_arrangement</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>hours_on_social_media</th>\n",
       "      <th>loneliness_score</th>\n",
       "      <th>social_media_frequency</th>\n",
       "      <th>mhi5_class_2022</th>\n",
       "      <th>gender.1</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Religious_Membership</th>\n",
       "      <th>political_interest</th>\n",
       "      <th>mhi5_std_score_2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>999</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender  sex  education  Employment_status  \\\n",
       "0  60.0     999  1.0        6.0                  1   \n",
       "1  32.0     999  2.0        4.0                  1   \n",
       "2  49.0     999  1.0        4.0                  1   \n",
       "3  70.0     999  1.0        6.0                  9   \n",
       "4  60.0     999  1.0        4.0                  4   \n",
       "\n",
       "   Personal_Net_Income_Category  Ethnic_Background  living_arrangement  \\\n",
       "0                           7.0                2.0                 3.0   \n",
       "1                           4.0                2.0                 3.0   \n",
       "2                           4.0                1.0                 2.0   \n",
       "3                           6.0                1.0                 2.0   \n",
       "4                           5.0                1.0                 3.0   \n",
       "\n",
       "   marital_status  hours_on_social_media  loneliness_score  \\\n",
       "0             1.0                    2.0                 0   \n",
       "1             1.0                   18.0                 0   \n",
       "2             1.0                   17.0                 0   \n",
       "3             1.0                    0.0                 0   \n",
       "4             1.0                    6.0                 0   \n",
       "\n",
       "   social_media_frequency  mhi5_class_2022  gender.1  Religion  \\\n",
       "0                     1.0              0.0       999       NaN   \n",
       "1                     3.0              1.0       999      10.0   \n",
       "2                     6.0              0.0       999       NaN   \n",
       "3                     5.0              0.0       999       NaN   \n",
       "4                     7.0              0.0       999       NaN   \n",
       "\n",
       "   Religious_Membership  political_interest  mhi5_std_score_2022  \n",
       "0                   2.0                 1.0                 84.0  \n",
       "1                   1.0                 2.0                 52.0  \n",
       "2                   2.0                 2.0                 64.0  \n",
       "3                   2.0                 1.0                 76.0  \n",
       "4                   2.0                 1.0                 88.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"social_demographic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eaf0a1-2818-42d9-990a-e02059a2cd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd685aa1-a75f-482e-8458-5f0f1a534d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the target variable\n",
    "target_column = ['mhi5_class_2022']\n",
    "\n",
    "# Columns that are inside blocks but MAY only be needed for error analysis\n",
    "possible_error_analysis_columns = ['gender', 'Religion', 'Religious_Membership', 'political_interest','mhi5_std_score_2022', 'gender.1', 'Ethnic_Background', 'social_media_frequency' ]\n",
    "\n",
    "# Full list of extra columns\n",
    "keep_columns = target_column + possible_error_analysis_columns \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=keep_columns) \n",
    "y = df[target_column[0]]  # Target variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e67ef68-679e-4ee9-afe1-7e3d19deead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X.reset_index(drop=True)\n",
    "y= y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d3c0313-0481-42a8-8913-cd0f38c34c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(995, 9)\n",
      "(995,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abc750fe-ca45-4cac-a832-0599260374dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                             0\n",
      "sex                             0\n",
      "education                       2\n",
      "Employment_status               0\n",
      "Personal_Net_Income_Category    2\n",
      "living_arrangement              0\n",
      "marital_status                  0\n",
      "hours_on_social_media           0\n",
      "loneliness_score                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum()) \n",
    "# the classidfer cannot work if some entries have NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64c5c6b9-637d-4d1d-9674-e9da354c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, \"education\"] = X[\"education\"].fillna(X[\"education\"].mean())\n",
    "X.loc[:, \"Personal_Net_Income_Category\"] = X[\"Personal_Net_Income_Category\"].fillna(X[\"Personal_Net_Income_Category\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "896aa3bf-d3be-4cd2-806f-5a5e745946f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                             0\n",
      "sex                             0\n",
      "education                       0\n",
      "Employment_status               0\n",
      "Personal_Net_Income_Category    0\n",
      "living_arrangement              0\n",
      "marital_status                  0\n",
      "hours_on_social_media           0\n",
      "loneliness_score                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum())  # check for nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b219d671-6220-4b0f-a085-e41024b8d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(y.isna().sum())\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136770e2-a51d-48e0-8f2d-a0fdd3104dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y.dropna(inplace=True)  #drop the entry with nan\n",
    "X = X.loc[y.index]    # keep X and y aligned\n",
    "print(y.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f109c-f0b9-49c5-9b01-da0ce0211c8f",
   "metadata": {},
   "source": [
    "Nested CROSS-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "593812e0-f4a0-4137-9ea1-6a5c5ab79508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1 - Train indices (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "Outer Fold 1 - Test indices (first 10): [10 23 25 29 30 39 44 55 59 60]\n",
      "\n",
      "========== Outer Fold 1 ==========\n",
      "\n",
      "PHASE 1: Finding best classifier and balancing technique combination with GridSearchCV\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 18 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 526, in fit\n    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 269\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# To use this function:\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m results \u001b[38;5;241m=\u001b[39m run_nested_cv(X, y)\n",
      "Cell \u001b[1;32mIn[34], line 148\u001b[0m, in \u001b[0;36mrun_nested_cv\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    139\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    140\u001b[0m     pipeline, \n\u001b[0;32m    141\u001b[0m     param_grid, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    145\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Train and find best hyperparameters\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Get the key for this classifier-balancing combination\u001b[39;00m\n\u001b[0;32m    151\u001b[0m model_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbal_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:996\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    994\u001b[0m     )\n\u001b[1;32m--> 996\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 18 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 526, in fit\n    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\u1246538\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define all models and their hyperparameter grids (simpler grids for GridSearchCV phase)\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000), {\n",
    "        'model__C': [0.0001, 0.001, 0.01, 1, 10, 100]\n",
    "    }),\n",
    "    'SVM': (SVC(), {\n",
    "        'model__C': [0.1, 1, 10, 100],\n",
    "        'model__kernel': ['linear', 'rbf'],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'model__n_estimators': [100],\n",
    "        'model__max_features': ['auto'],\n",
    "        'model__max_depth': [None],\n",
    "        'model__min_samples_split': [2],\n",
    "        'model__min_samples_leaf': [1]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'model__n_estimators': [100],\n",
    "        'model__learning_rate': [0.1],\n",
    "        'model__max_depth': [6],\n",
    "        'model__subsample': [0.8],\n",
    "        'model__colsample_bytree': [0.8]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Define balancing techniques\n",
    "balancing_techniques = {\n",
    "    'None': None,\n",
    "    'SMOTE-Tomek': SMOTETomek(),\n",
    "    'Random Undersampling': RandomUnderSampler()\n",
    "}\n",
    "\n",
    "# Function for Optuna optimization for finer hyperparameter tuning\n",
    "def optuna_objective(trial, model_name, X, y, balancing_method, cv):\n",
    "    if model_name == 'Random Forest':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators', [50, 100, 150, 200, 250, 300]),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "            'max_depth': trial.suggest_categorical('max_depth', [None, 5, 10, 15, 20, 25, 30]),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample'])\n",
    "        }\n",
    "        base_model = RandomForestClassifier(**params)\n",
    "    \n",
    "    elif model_name == 'XGBoost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators', [50, 100, 200, 300, 400, 500]),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10)\n",
    "        }\n",
    "        base_model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')\n",
    "    \n",
    "    # Create a pipeline with the balancer and model\n",
    "    if balancing_method is None:\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', base_model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = ImbPipeline([\n",
    "            ('balancer', balancing_method),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', base_model)\n",
    "        ])\n",
    "    \n",
    "    # Cross-validate\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=cv)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict(X_val)\n",
    "        scores.append(f1_score(y_val, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def run_nested_cv(X, y):\n",
    "    # Outer loop: 5-Fold Nested Cross Validation\n",
    "    outer_kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_kf.split(X), 1):\n",
    "        print(f\"Outer Fold {outer_fold} - Train indices (first 10): {train_idx[:10]}\")\n",
    "        print(f\"Outer Fold {outer_fold} - Test indices (first 10): {test_idx[:10]}\")\n",
    "    \n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Inner loop: 3-Fold Cross Validation for model selection & hyperparameter tuning\n",
    "        inner_kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        print(f\"\\n========== Outer Fold {outer_fold} ==========\")\n",
    "        print(\"\\nPHASE 1: Finding best classifier and balancing technique combination with GridSearchCV\")\n",
    "        \n",
    "        # Dictionary to store the best model for each classifier-balancing combination\n",
    "        best_models_for_fold = {}\n",
    "        grid_search_results = {}\n",
    "        \n",
    "        # PHASE 1: Test all combinations of classifiers and balancing techniques with GridSearchCV\n",
    "        for bal_name, bal_method in balancing_techniques.items():\n",
    "            for model_name, (model, param_grid) in models.items():\n",
    "                # Create pipeline with balancing technique\n",
    "                pipeline = ImbPipeline([\n",
    "                    ('balancer', bal_method),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('model', model)\n",
    "                ])\n",
    "                \n",
    "                # Perform GridSearchCV\n",
    "                grid_search = GridSearchCV(\n",
    "                    pipeline, \n",
    "                    param_grid, \n",
    "                    cv=inner_kf, \n",
    "                    scoring='f1', \n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Train and find best hyperparameters\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "                # Get the key for this classifier-balancing combination\n",
    "                model_key = f\"{model_name} ({bal_name})\"\n",
    "                \n",
    "                # Store the best model for this combination\n",
    "                best_models_for_fold[model_key] = grid_search.best_estimator_\n",
    "                grid_search_results[model_key] = grid_search.best_score_\n",
    "                \n",
    "                # Evaluate on the test set\n",
    "                y_pred = grid_search.predict(X_test)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                results[f\"GridSearch-{model_key}\"].append(f1)\n",
    "                \n",
    "                # Print result for this fold and model\n",
    "                print(f\"  {model_key}: F1 = {f1:.4f} (Inner CV best: {grid_search.best_score_:.4f})\")\n",
    "        \n",
    "        # Find the best classifier and balancing technique combination\n",
    "        best_combo = max(grid_search_results.items(), key=lambda x: x[1])\n",
    "        best_combo_name = best_combo[0]\n",
    "        best_combo_score = best_combo[1]\n",
    "        \n",
    "        print(f\"\\nBest combination from GridSearchCV: {best_combo_name} with inner CV F1 = {best_combo_score:.4f}\")\n",
    "        \n",
    "        # Extract classifier name and balancing technique from the best combination\n",
    "        best_classifier = best_combo_name.split(' (')[0]\n",
    "        best_balancer_name = best_combo_name.split('(')[1].replace(')', '')\n",
    "        best_balancer = balancing_techniques[best_balancer_name]\n",
    "        \n",
    "        # PHASE 2: If the best classifier is RF or XGBoost, use Optuna for finer hyperparameter tuning\n",
    "        if best_classifier in ['Random Forest', 'XGBoost']:\n",
    "            print(f\"\\nPHASE 2: Using Optuna for finer hyperparameter tuning of {best_classifier} with {best_balancer_name}\")\n",
    "            \n",
    "            # Create and optimize the Optuna study\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: optuna_objective(\n",
    "                    trial, \n",
    "                    best_classifier, \n",
    "                    X_train, \n",
    "                    y_train, \n",
    "                    best_balancer, \n",
    "                    cv=3\n",
    "                ), \n",
    "                n_trials=30\n",
    "            )\n",
    "            \n",
    "            # Get best parameters\n",
    "            best_params = study.best_params\n",
    "            print(f\"  Best parameters found by Optuna: {best_params}\")\n",
    "            \n",
    "            # Create model with best parameters\n",
    "            if best_classifier == 'Random Forest':\n",
    "                best_model = RandomForestClassifier(**best_params)\n",
    "            else:  # XGBoost\n",
    "                best_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
    "            \n",
    "            # Create pipeline with best balancing technique and optimized model\n",
    "            if best_balancer is None:\n",
    "                final_pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('model', best_model)\n",
    "                ])\n",
    "            else:\n",
    "                final_pipeline = ImbPipeline([\n",
    "                    ('balancer', best_balancer),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('model', best_model)\n",
    "                ])\n",
    "            \n",
    "            # Fit the optimized model\n",
    "            final_pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = final_pipeline.predict(X_test)\n",
    "            optuna_f1 = f1_score(y_test, y_pred)\n",
    "            results[f\"Optuna-{best_classifier} ({best_balancer_name})\"].append(optuna_f1)\n",
    "            \n",
    "            print(f\"  Optuna-optimized {best_classifier} with {best_balancer_name}: F1 = {optuna_f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\nPHASE 2: Skipping Optuna since best classifier is {best_classifier}, not RF or XGBoost\")\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n========== Final Results ==========\")\n",
    "    print(\"\\nMean F1 Scores Across All Outer Folds:\")\n",
    "    \n",
    "    # Group results by GridSearch and Optuna\n",
    "    grid_results = {k: v for k, v in results.items() if k.startswith('GridSearch')}\n",
    "    optuna_results = {k: v for k, v in results.items() if k.startswith('Optuna')}\n",
    "    \n",
    "    print(\"\\nGridSearchCV Results:\")\n",
    "    for model_name, scores in grid_results.items():\n",
    "        if len(scores) > 0:  # Only print if we have results\n",
    "            mean_f1 = np.mean(scores)\n",
    "            std_f1 = np.std(scores)\n",
    "            model_name_clean = model_name.replace('GridSearch-', '')\n",
    "            print(f\"  {model_name_clean}: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    \n",
    "    if optuna_results:\n",
    "        print(\"\\nOptuna Results:\")\n",
    "        for model_name, scores in optuna_results.items():\n",
    "            if len(scores) > 0:  # Only print if we have results\n",
    "                mean_f1 = np.mean(scores)\n",
    "                std_f1 = np.std(scores)\n",
    "                model_name_clean = model_name.replace('Optuna-', '')\n",
    "                print(f\"  {model_name_clean}: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    \n",
    "    # Find the best overall model across all methods\n",
    "    all_results = {}\n",
    "    for model_type, results_dict in [(\"GridSearch\", grid_results), (\"Optuna\", optuna_results)]:\n",
    "        for model_name, scores in results_dict.items():\n",
    "            if len(scores) > 0:\n",
    "                all_results[model_name] = np.mean(scores)\n",
    "    \n",
    "    if all_results:\n",
    "        best_model_name = max(all_results.items(), key=lambda x: x[1])[0]\n",
    "        print(f\"\\nBest overall model: {best_model_name} with mean F1 = {all_results[best_model_name]:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# To use this function:\n",
    "results = run_nested_cv(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0a98a-c853-44b2-bb9a-aae83be4c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use this function:\n",
    "results = run_nested_cv(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b67ba-32bd-4491-80cf-1cc67e496da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
